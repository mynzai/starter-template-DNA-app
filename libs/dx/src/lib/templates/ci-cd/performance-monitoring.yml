# Performance Monitoring and Regression Detection CI/CD Workflow
# Comprehensive performance testing with automated baseline comparison

name: Performance Monitoring

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance tests daily at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      create_baseline:
        description: 'Create new performance baseline'
        required: false
        default: 'false'
        type: boolean
      target_environment:
        description: 'Target environment for testing'
        required: false
        default: 'staging'
        type: choice
        options:
        - staging
        - production
        - development

env:
  NODE_VERSION: '18'
  PERFORMANCE_TIMEOUT: '600000' # 10 minutes
  REGRESSION_THRESHOLD: '0.1' # 10% regression threshold

jobs:
  # Environment Setup and Detection
  setup:
    name: Setup Performance Testing
    runs-on: ubuntu-latest
    outputs:
      template-type: ${{ steps.detect-template.outputs.type }}
      framework: ${{ steps.detect-template.outputs.framework }}
      has-web: ${{ steps.detect-template.outputs.has-web }}
      has-api: ${{ steps.detect-template.outputs.has-api }}
      has-mobile: ${{ steps.detect-template.outputs.has-mobile }}
      baseline-exists: ${{ steps.check-baseline.outputs.exists }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect template type and framework
        id: detect-template
        run: |
          # Detect template type based on project structure
          if [ -f "next.config.js" ] || [ -f "next.config.ts" ]; then
            echo "type=ai-saas" >> $GITHUB_OUTPUT
            echo "framework=nextjs" >> $GITHUB_OUTPUT
            echo "has-web=true" >> $GITHUB_OUTPUT
            echo "has-api=true" >> $GITHUB_OUTPUT
          elif [ -f "src-tauri/Cargo.toml" ]; then
            echo "type=high-performance" >> $GITHUB_OUTPUT
            echo "framework=tauri" >> $GITHUB_OUTPUT
            echo "has-web=true" >> $GITHUB_OUTPUT
            echo "has-api=true" >> $GITHUB_OUTPUT
          elif [ -f "pubspec.yaml" ]; then
            echo "type=cross-platform" >> $GITHUB_OUTPUT
            echo "framework=flutter" >> $GITHUB_OUTPUT
            echo "has-mobile=true" >> $GITHUB_OUTPUT
            echo "has-api=true" >> $GITHUB_OUTPUT  
          elif [ -f "svelte.config.js" ]; then
            echo "type=data-visualization" >> $GITHUB_OUTPUT
            echo "framework=sveltekit" >> $GITHUB_OUTPUT
            echo "has-web=true" >> $GITHUB_OUTPUT
          elif [ -f "package.json" ] && grep -q "react-native" package.json; then
            echo "type=cross-platform" >> $GITHUB_OUTPUT
            echo "framework=react-native" >> $GITHUB_OUTPUT
            echo "has-mobile=true" >> $GITHUB_OUTPUT
          else
            echo "type=generic" >> $GITHUB_OUTPUT
            echo "framework=unknown" >> $GITHUB_OUTPUT
            echo "has-web=false" >> $GITHUB_OUTPUT
            echo "has-api=false" >> $GITHUB_OUTPUT
            echo "has-mobile=false" >> $GITHUB_OUTPUT
          fi

      - name: Check existing performance baseline
        id: check-baseline
        run: |
          TEMPLATE_TYPE="${{ steps.detect-template.outputs.type }}"
          if [ -f "performance-baselines/${TEMPLATE_TYPE}-baseline.json" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

  # Performance Benchmarking
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: setup
    strategy:
      matrix:
        test-type: [api, web, mobile, system]
        exclude:
          - test-type: web
            condition: ${{ needs.setup.outputs.has-web != 'true' }}
          - test-type: api  
            condition: ${{ needs.setup.outputs.has-api != 'true' }}
          - test-type: mobile
            condition: ${{ needs.setup.outputs.has-mobile != 'true' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm ci
          npm install -g k6 artillery lighthouse-ci autocannon

      - name: Build application
        run: |
          if [ "${{ needs.setup.outputs.framework }}" = "nextjs" ]; then
            npm run build
          elif [ "${{ needs.setup.outputs.framework }}" = "sveltekit" ]; then
            npm run build
          elif [ "${{ needs.setup.outputs.framework }}" = "flutter" ]; then
            flutter build web
          fi

      - name: Start application servers
        run: |
          # Start application based on framework
          case "${{ needs.setup.outputs.framework }}" in
            "nextjs")
              npm start &
              ;;
            "sveltekit") 
              npm run preview &
              ;;
            "flutter")
              python3 -m http.server 8080 -d build/web &
              ;;
            *)
              npm start &
              ;;
          esac
          
          # Wait for services to be ready
          npx wait-on http://localhost:3000 --timeout ${{ env.PERFORMANCE_TIMEOUT }} || \
          npx wait-on http://localhost:8080 --timeout ${{ env.PERFORMANCE_TIMEOUT }}

      - name: Run API Performance Tests
        if: matrix.test-type == 'api' && needs.setup.outputs.has-api == 'true'
        run: |
          # K6 load testing
          k6 run --out json=k6-results.json tools/performance-testing/load-tests/api-load-test.js
          
          # Artillery stress testing  
          artillery run tools/performance-testing/load-tests/api-stress-test.yml -o artillery-results.json
          
          # Autocannon rapid testing
          autocannon -c 100 -d 30 -j http://localhost:3000/api/health > autocannon-results.json

      - name: Run Web Performance Tests
        if: matrix.test-type == 'web' && needs.setup.outputs.has-web == 'true'
        run: |
          # Lighthouse performance audit
          lhci autorun --config=tools/performance-testing/configs/lighthouse-ci.json
          
          # Custom web vitals testing
          node tools/performance-testing/web-vitals/measure-vitals.js > web-vitals-results.json

      - name: Run Mobile Performance Tests
        if: matrix.test-type == 'mobile' && needs.setup.outputs.has-mobile == 'true'
        run: |
          # Flutter mobile performance testing
          if [ "${{ needs.setup.outputs.framework }}" = "flutter" ]; then
            flutter test integration_test/performance_test.dart --profile > flutter-perf-results.json
          fi
          
          # React Native performance testing
          if [ "${{ needs.setup.outputs.framework }}" = "react-native" ]; then
            npx detox test -c ios.sim.release --headless > rn-perf-results.json
          fi

      - name: Run System Performance Tests
        if: matrix.test-type == 'system'
        run: |
          # System resource monitoring during load
          node tools/performance-testing/monitoring/system-monitor.js &
          MONITOR_PID=$!
          
          # Run concurrent load while monitoring
          sleep 30
          kill $MONITOR_PID
          
          # Collect system metrics
          mv system-metrics.json system-perf-results.json

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results-${{ matrix.test-type }}
          path: |
            *-results.json
            lighthouse-report.html
            .lighthouseci/
          retention-days: 30

  # Regression Detection and Analysis
  regression-analysis:
    name: Performance Regression Analysis
    runs-on: ubuntu-latest
    needs: [setup, benchmark]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install analysis dependencies
        run: |
          npm ci
          npm install -g jq

      - name: Download benchmark artifacts
        uses: actions/download-artifact@v3
        with:
          path: ./performance-results

      - name: Aggregate performance data
        run: |
          node tools/performance-testing/analysis/aggregate-results.js \
            --input-dir=./performance-results \
            --output=aggregated-results.json \
            --template-type=${{ needs.setup.outputs.template-type }}

      - name: Compare against baseline
        id: regression-check
        run: |
          TEMPLATE_TYPE="${{ needs.setup.outputs.template-type }}"
          
          if [ "${{ needs.setup.outputs.baseline-exists }}" = "true" ]; then
            # Run regression analysis
            node tools/performance-testing/regression-tests/baseline-manager.js compare \
              --template-type=${TEMPLATE_TYPE} \
              --current-results=aggregated-results.json \
              --tolerance=${{ env.REGRESSION_THRESHOLD }} \
              --output=regression-report.json
            
            # Check if regressions were found
            HAS_REGRESSIONS=$(jq -r '.passed == false' regression-report.json)
            echo "has-regressions=${HAS_REGRESSIONS}" >> $GITHUB_OUTPUT
            
            # Extract regression count
            REGRESSION_COUNT=$(jq -r '.issues | length' regression-report.json)
            echo "regression-count=${REGRESSION_COUNT}" >> $GITHUB_OUTPUT
          else
            echo "has-regressions=false" >> $GITHUB_OUTPUT
            echo "regression-count=0" >> $GITHUB_OUTPUT
          fi

      - name: Create baseline if requested or missing
        if: github.event.inputs.create_baseline == 'true' || needs.setup.outputs.baseline-exists == 'false'
        run: |
          TEMPLATE_TYPE="${{ needs.setup.outputs.template-type }}"
          
          # Create new baseline
          node tools/performance-testing/regression-tests/baseline-manager.js create \
            --template-type=${TEMPLATE_TYPE} \
            --test-results=aggregated-results.json \
            --version=${{ github.sha }} \
            --environment=${{ github.event.inputs.target_environment || 'staging' }}
          
          # Commit baseline to repository
          git config user.name "Performance Bot"
          git config user.email "performance@example.com"
          git add performance-baselines/${TEMPLATE_TYPE}-baseline.json
          git commit -m "feat: create performance baseline for ${TEMPLATE_TYPE} [skip ci]"
          git push

      - name: Generate performance report
        run: |
          node tools/performance-testing/analysis/generate-report.js \
            --results=aggregated-results.json \
            --regression-report=regression-report.json \
            --template-type=${{ needs.setup.outputs.template-type }} \
            --output-format=markdown \
            --output=performance-report.md

      - name: Upload analysis results
        uses: actions/upload-artifact@v3
        with:
          name: regression-analysis
          path: |
            regression-report.json
            performance-report.md
            aggregated-results.json

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = './performance-report.md';
            if (fs.existsSync(path)) {
              const report = fs.readFileSync(path, 'utf8');
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## 📊 Performance Test Results\n\n${report}`
              });
            }

  # Performance Alerting
  alerting:
    name: Performance Alerting
    runs-on: ubuntu-latest
    needs: [setup, regression-analysis]
    if: needs.regression-analysis.outputs.has-regressions == 'true'
    steps:
      - name: Download regression analysis
        uses: actions/download-artifact@v3
        with:
          name: regression-analysis

      - name: Send Slack alert for regressions
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          channel: '#performance-alerts'
          text: |
            🚨 Performance Regression Detected!
            
            Template: ${{ needs.setup.outputs.template-type }}
            Framework: ${{ needs.setup.outputs.framework }}
            Regressions: ${{ needs.regression-analysis.outputs.regression-count }}
            Commit: ${{ github.sha }}
            
            Review the performance report for details.
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}

      - name: Create GitHub issue for regression
        if: github.ref == 'refs/heads/main'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const regressionReport = JSON.parse(fs.readFileSync('regression-report.json', 'utf8'));
            
            const issueBody = `
            ## 🚨 Performance Regression Detected
            
            **Template Type:** ${{ needs.setup.outputs.template-type }}
            **Framework:** ${{ needs.setup.outputs.framework }}
            **Commit:** ${{ github.sha }}
            **Regression Count:** ${{ needs.regression-analysis.outputs.regression-count }}
            
            ### Issues Found:
            ${regressionReport.issues.map(issue => 
              `- **${issue.metric}**: ${issue.severity} severity - ${issue.message}`
            ).join('\n')}
            
            ### Recommendations:
            ${regressionReport.recommendations?.map(rec => 
              `- **${rec.type}**: ${rec.message}`
            ).join('\n') || 'See full analysis for recommendations'}
            
            Please investigate and resolve these performance regressions.
            `;
            
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Performance Regression: ${{ needs.setup.outputs.template-type }} - ${{ github.sha }}`,
              body: issueBody,
              labels: ['performance', 'regression', 'priority-high']
            });

  # Performance Monitoring Dashboard Update
  monitoring-update:
    name: Update Performance Dashboard
    runs-on: ubuntu-latest
    needs: [setup, regression-analysis]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download analysis results
        uses: actions/download-artifact@v3
        with:
          name: regression-analysis

      - name: Update performance metrics
        run: |
          # Send metrics to monitoring system (Prometheus/Grafana)
          curl -X POST "${{ secrets.PROMETHEUS_PUSHGATEWAY_URL }}/metrics/job/performance-ci/instance/${{ needs.setup.outputs.template-type }}" \
            --data-binary @performance-metrics.txt || echo "Metrics push failed"

      - name: Update dashboard data
        run: |
          # Update performance dashboard with latest results
          node tools/performance-testing/monitoring/update-dashboard.js \
            --results=aggregated-results.json \
            --template-type=${{ needs.setup.outputs.template-type }} \
            --commit=${{ github.sha }} \
            --branch=${{ github.ref_name }}

  # Quality Gate Decision
  performance-gate:
    name: Performance Quality Gate
    runs-on: ubuntu-latest
    needs: [setup, regression-analysis]
    if: always()
    steps:
      - name: Evaluate performance gate
        run: |
          echo "Performance Gate Evaluation"
          echo "Template: ${{ needs.setup.outputs.template-type }}"
          echo "Has Regressions: ${{ needs.regression-analysis.outputs.has-regressions }}"
          echo "Regression Count: ${{ needs.regression-analysis.outputs.regression-count }}"
          
          # Fail the gate if critical regressions are found
          if [ "${{ needs.regression-analysis.outputs.has-regressions }}" = "true" ]; then
            CRITICAL_REGRESSIONS=$(jq -r '.issues | map(select(.severity == "critical")) | length' regression-report.json || echo "0")
            
            if [ "$CRITICAL_REGRESSIONS" -gt "0" ]; then
              echo "❌ Performance gate FAILED: $CRITICAL_REGRESSIONS critical regressions found"
              exit 1
            else
              echo "⚠️  Performance gate PASSED with warnings: Non-critical regressions found"
            fi
          else
            echo "✅ Performance gate PASSED: No regressions detected"
          fi